{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle data\n",
    "# https://www.kaggle.com/competitions/titanic/data\n",
    "# \n",
    "# # !pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 9)\n",
      "   Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  \\\n",
      "0         0       3  22.0      1      0   7.2500      True       False   \n",
      "1         1       1  38.0      1      0  71.2833     False       False   \n",
      "2         1       3  26.0      0      0   7.9250     False       False   \n",
      "3         1       1  35.0      1      0  53.1000     False       False   \n",
      "4         0       3  35.0      0      0   8.0500      True       False   \n",
      "\n",
      "   Embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Titanic_train_preprocessed.csv\")\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data are too small for training the model, but at the same time too simple for training. Thus, lets split it into a 90% training, 5% validation and 5% testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 640 samples\n",
      "Validation set: 36 samples\n",
      "Testing set: 36 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Titanic_train_preprocessed.csv\")\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data.drop('Survived', axis=1)  # Features\n",
    "y = data['Survived']               # Target variable\n",
    "\n",
    "# Step 1: Split the data into 90% training and 10% temporary\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Step 2: Split the temporary set into 50% validation and 50% testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes to ensure correct splitting\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 1, 'penalty': 'l2'}\n",
      "Accuracy: 0.86\n",
      "Precision: 0.92\n",
      "Recall: 0.73\n",
      "F1 Score: 0.81\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.67      0.62        21\n",
      "           1       0.42      0.33      0.37        15\n",
      "\n",
      "    accuracy                           0.53        36\n",
      "   macro avg       0.50      0.50      0.50        36\n",
      "weighted avg       0.51      0.53      0.52        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "90 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [      nan 0.66875         nan       nan       nan 0.71875         nan\n",
      "       nan       nan 0.8             nan       nan       nan 0.8078125\n",
      "       nan       nan       nan 0.7984375       nan       nan       nan\n",
      " 0.803125        nan       nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "    # 'max_iter': [100, 200, 300],\n",
    "    # 'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the desired scoring metric\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Train the Logistic Regression model with the best hyperparameters\n",
    "best_log_reg = LogisticRegression(**best_params, random_state=42)\n",
    "best_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Replace this with your actual trained model variable\n",
    "model = best_log_reg\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "LR_accuracy = accuracy_score(y_test, y_pred)\n",
    "LR_precision = precision_score(y_test, y_pred)\n",
    "LR_recall = recall_score(y_test, y_pred)\n",
    "LR_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {LR_accuracy:.2f}\")\n",
    "print(f\"Precision: {LR_precision:.2f}\")\n",
    "print(f\"Recall: {LR_recall:.2f}\")\n",
    "print(f\"F1 Score: {LR_f1:.2f}\")\n",
    "\n",
    "# Print a comprehensive classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 18:08:33,614] A new study created in memory with name: no-name-6bcca4a1-3a36-48ee-99ff-e9d60e7b641e\n",
      "[I 2024-09-17 18:08:34,167] Trial 0 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 271, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 0 with value: 0.7777777777777778.\n",
      "[I 2024-09-17 18:08:34,646] Trial 1 finished with value: 0.75 and parameters: {'n_estimators': 276, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.7777777777777778.\n",
      "[I 2024-09-17 18:08:34,839] Trial 2 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 111, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:35,377] Trial 3 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 271, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:35,741] Trial 4 finished with value: 0.75 and parameters: {'n_estimators': 154, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:36,231] Trial 5 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 228, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:36,472] Trial 6 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 118, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:36,835] Trial 7 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 197, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:37,185] Trial 8 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 222, 'max_depth': 30, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:37,465] Trial 9 finished with value: 0.75 and parameters: {'n_estimators': 141, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:37,716] Trial 10 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:38,050] Trial 11 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 212, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:38,404] Trial 12 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 174, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:38,791] Trial 13 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 226, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:39,189] Trial 14 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 247, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:39,490] Trial 15 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 186, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:39,925] Trial 16 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 247, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:40,570] Trial 17 finished with value: 0.75 and parameters: {'n_estimators': 295, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:40,865] Trial 18 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 164, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:41,106] Trial 19 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 132, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:41,622] Trial 20 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 242, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:41,958] Trial 21 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 195, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:42,306] Trial 22 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 202, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:42,506] Trial 23 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 109, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:42,791] Trial 24 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 178, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:43,051] Trial 25 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 149, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:43,272] Trial 26 finished with value: 0.75 and parameters: {'n_estimators': 126, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:43,656] Trial 27 finished with value: 0.75 and parameters: {'n_estimators': 231, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:44,091] Trial 28 finished with value: 0.75 and parameters: {'n_estimators': 207, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:44,557] Trial 29 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 262, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:45,155] Trial 30 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 298, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:45,527] Trial 31 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 215, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:45,855] Trial 32 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 194, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:46,306] Trial 33 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 235, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:46,817] Trial 34 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 259, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:47,289] Trial 35 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 285, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:47,626] Trial 36 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 217, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:47,924] Trial 37 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 162, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:48,277] Trial 38 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 213, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:48,749] Trial 39 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 257, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:49,213] Trial 40 finished with value: 0.75 and parameters: {'n_estimators': 275, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:49,515] Trial 41 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 178, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:49,825] Trial 42 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 174, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:50,156] Trial 43 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 190, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:50,538] Trial 44 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 225, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:50,940] Trial 45 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 207, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:51,239] Trial 46 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 145, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:51,573] Trial 47 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 163, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:51,798] Trial 48 finished with value: 0.8055555555555556 and parameters: {'n_estimators': 118, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8055555555555556.\n",
      "[I 2024-09-17 18:08:52,202] Trial 49 finished with value: 0.75 and parameters: {'n_estimators': 238, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8055555555555556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 111, 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}\n",
      "Best accuracy: 0.8055555555555556\n",
      "Accuracy: 0.81\n",
      "Precision: 0.90\n",
      "Recall: 0.60\n",
      "F1 Score: 0.72\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85        21\n",
      "           1       0.90      0.60      0.72        15\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.83      0.78      0.79        36\n",
      "weighted avg       0.82      0.81      0.80        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 10, 30, step=5)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "    # Initialize the Random Forest model with the hyperparameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Create the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "# Train the Random Forest Classifier with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using the specified metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "RF_accuracy = accuracy_score(y_test, y_pred)\n",
    "RF_precision = precision_score(y_test, y_pred)\n",
    "RF_recall = recall_score(y_test, y_pred)\n",
    "RF_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {RF_accuracy:.2f}\")\n",
    "print(f\"Precision: {RF_precision:.2f}\")\n",
    "print(f\"Recall: {RF_recall:.2f}\")\n",
    "print(f\"F1 Score: {RF_f1:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best hyperparameters found: {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "Best score: 0.78\n",
      "Time taken for Random Search: 19.69 seconds\n",
      "Validation Accuracy: 0.78\n",
      "Validation Precision: 0.82\n",
      "Validation Recall: 0.60\n",
      "Validation F1 Score: 0.69\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70        21\n",
      "           1       0.55      0.40      0.46        15\n",
      "\n",
      "    accuracy                           0.61        36\n",
      "   macro avg       0.59      0.58      0.58        36\n",
      "weighted avg       0.60      0.61      0.60        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"Titanic_train_preprocessed.csv\")\n",
    "\n",
    "# Define feature columns and target\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the data into training (90%), validation (5%), and testing (5%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a simplified parameter grid for Random Search\n",
    "param_dist = {\n",
    "    'C': [0.1, 1, 10],                # Simplified to 3 values\n",
    "    'kernel': ['linear', 'rbf'],       # Reduced to 2 commonly used kernels\n",
    "    'gamma': [0.001, 0.01, 0.1]        # Simplified to 3 values\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with fewer iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svc,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,   # Reduced number of random combinations to search\n",
    "    cv=3,        # Reduced the number of cross-validation folds for faster computation\n",
    "    n_jobs=-1,   # Use all available cores\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Timing the Random Search\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform Random Search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best hyperparameters found: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_:.2f}\")\n",
    "print(f\"Time taken for Random Search: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Train the SVM model with the best hyperparameters\n",
    "best_svc = random_search.best_estimator_\n",
    "best_svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "svm_accuracy = accuracy_score(y_test, y_pred)\n",
    "svm_precision = precision_score(y_test, y_pred)\n",
    "svm_recall = recall_score(y_test, y_pred)\n",
    "svm_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Validation Accuracy: {svm_accuracy:.2f}\")\n",
    "print(f\"Validation Precision: {svm_precision:.2f}\")\n",
    "print(f\"Validation Recall: {svm_recall:.2f}\")\n",
    "print(f\"Validation F1 Score: {svm_f1:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 18:09:51,725] A new study created in memory with name: no-name-66d82442-805b-4339-b5ad-bcb44457e109\n",
      "[I 2024-09-17 18:09:51,734] Trial 0 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 0 with value: 0.6944444444444444.\n",
      "[I 2024-09-17 18:09:51,745] Trial 1 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2}. Best is trial 0 with value: 0.6944444444444444.\n",
      "[I 2024-09-17 18:09:51,803] Trial 2 finished with value: 0.75 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,813] Trial 3 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,847] Trial 4 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 42, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,867] Trial 5 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,874] Trial 6 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,936] Trial 7 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'algorithm': 'brute', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,944] Trial 8 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:51,961] Trial 9 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 38, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,017] Trial 10 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,069] Trial 11 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,116] Trial 12 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,162] Trial 13 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 31, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,181] Trial 14 finished with value: 0.75 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,200] Trial 15 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,233] Trial 16 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,256] Trial 17 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,281] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,307] Trial 19 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,369] Trial 20 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,420] Trial 21 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,469] Trial 22 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,530] Trial 23 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,581] Trial 24 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 43, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,606] Trial 25 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 33, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,632] Trial 26 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,662] Trial 27 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,715] Trial 28 finished with value: 0.75 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,735] Trial 29 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,762] Trial 30 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,816] Trial 31 finished with value: 0.6111111111111112 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,867] Trial 32 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 35, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,918] Trial 33 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,965] Trial 34 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:52,982] Trial 35 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 41, 'weights': 'distance', 'algorithm': 'auto', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,003] Trial 36 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,054] Trial 37 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'brute', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,112] Trial 38 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'algorithm': 'brute', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,134] Trial 39 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 28, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,164] Trial 40 finished with value: 0.75 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,191] Trial 41 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,214] Trial 42 finished with value: 0.75 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,238] Trial 43 finished with value: 0.75 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,282] Trial 44 finished with value: 0.75 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,307] Trial 45 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,335] Trial 46 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,363] Trial 47 finished with value: 0.7222222222222222 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,385] Trial 48 finished with value: 0.6666666666666666 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n",
      "[I 2024-09-17 18:09:53,412] Trial 49 finished with value: 0.6944444444444444 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 2 with value: 0.75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_neighbors': 16, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2}\n",
      "Best accuracy: 0.75\n",
      "Accuracy: 0.56\n",
      "Precision: 0.46\n",
      "Recall: 0.40\n",
      "F1 Score: 0.43\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64        21\n",
      "           1       0.46      0.40      0.43        15\n",
      "\n",
      "    accuracy                           0.56        36\n",
      "   macro avg       0.54      0.53      0.53        36\n",
      "weighted avg       0.55      0.56      0.55        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to be tuned\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 3, 50)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "    p = trial.suggest_int('p', 1, 2)  # p=1 for Manhattan distance, p=2 for Euclidean distance\n",
    "\n",
    "    # Initialize the KNN model with the hyperparameters\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=weights,\n",
    "        algorithm=algorithm,\n",
    "        p=p\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Create the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)\n",
    "\n",
    "# Train the KNN model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = best_knn.predict(X_val)\n",
    "\n",
    "# Evaluate the model using the specified metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, y_pred)\n",
    "knn_precision = precision_score(y_test, y_pred)\n",
    "knn_recall = recall_score(y_test, y_pred)\n",
    "knn_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {knn_accuracy:.2f}\")\n",
    "print(f\"Precision: {knn_precision:.2f}\")\n",
    "print(f\"Recall: {knn_recall:.2f}\")\n",
    "print(f\"F1 Score: {knn_f1:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Method  Accuracy  precision    Recall  F1 Score\n",
      "0  Logistric Regression  0.861111   0.916667  0.733333  0.814815\n",
      "1         Random forest  0.805556   0.900000  0.600000  0.720000\n",
      "2                   SVM  0.777778   0.818182  0.600000  0.692308\n",
      "3                   KNN  0.555556   0.461538  0.400000  0.428571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_52df1\">\n",
       "  <caption>Regression Models Performance Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_52df1_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
       "      <th id=\"T_52df1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_52df1_level0_col2\" class=\"col_heading level0 col2\" >precision</th>\n",
       "      <th id=\"T_52df1_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_52df1_level0_col4\" class=\"col_heading level0 col4\" >F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_52df1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_52df1_row0_col0\" class=\"data row0 col0\" >Logistric Regression</td>\n",
       "      <td id=\"T_52df1_row0_col1\" class=\"data row0 col1\" >0.86</td>\n",
       "      <td id=\"T_52df1_row0_col2\" class=\"data row0 col2\" >0.92</td>\n",
       "      <td id=\"T_52df1_row0_col3\" class=\"data row0 col3\" >0.73</td>\n",
       "      <td id=\"T_52df1_row0_col4\" class=\"data row0 col4\" >0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52df1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_52df1_row1_col0\" class=\"data row1 col0\" >Random forest</td>\n",
       "      <td id=\"T_52df1_row1_col1\" class=\"data row1 col1\" >0.81</td>\n",
       "      <td id=\"T_52df1_row1_col2\" class=\"data row1 col2\" >0.90</td>\n",
       "      <td id=\"T_52df1_row1_col3\" class=\"data row1 col3\" >0.60</td>\n",
       "      <td id=\"T_52df1_row1_col4\" class=\"data row1 col4\" >0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52df1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_52df1_row2_col0\" class=\"data row2 col0\" >SVM</td>\n",
       "      <td id=\"T_52df1_row2_col1\" class=\"data row2 col1\" >0.78</td>\n",
       "      <td id=\"T_52df1_row2_col2\" class=\"data row2 col2\" >0.82</td>\n",
       "      <td id=\"T_52df1_row2_col3\" class=\"data row2 col3\" >0.60</td>\n",
       "      <td id=\"T_52df1_row2_col4\" class=\"data row2 col4\" >0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_52df1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_52df1_row3_col0\" class=\"data row3 col0\" >KNN</td>\n",
       "      <td id=\"T_52df1_row3_col1\" class=\"data row3 col1\" >0.56</td>\n",
       "      <td id=\"T_52df1_row3_col2\" class=\"data row3 col2\" >0.46</td>\n",
       "      <td id=\"T_52df1_row3_col3\" class=\"data row3 col3\" >0.40</td>\n",
       "      <td id=\"T_52df1_row3_col4\" class=\"data row3 col4\" >0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c1824c0260>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace these with the actual numeric (float) values from your experiments\n",
    "metrics_data = {\n",
    "    'Method': ['Logistric Regression', 'Random forest', 'SVM', 'KNN'],\n",
    "    'Accuracy': [LR_accuracy, RF_accuracy, svm_accuracy, knn_accuracy],  # Ensure these are float values\n",
    "    'precision': [LR_precision, RF_precision, svm_precision, knn_precision],        # Ensure these are float values\n",
    "    'Recall': [LR_recall, RF_recall, svm_recall, knn_recall],        # Ensure these are float values\n",
    "    'F1 Score': [LR_f1,RF_f1,svm_f1,knn_f1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Print the table\n",
    "print(metrics_df)\n",
    "\n",
    "# If you want to format the table with two decimal places, use this:\n",
    "metrics_df = metrics_df.style.format({\"Accuracy\": \"{:.2f}\", \"precision\": \"{:.2f}\", \"Recall\": \"{:.2f}\", \"F1 Score\": \"{:.2f}\"})\n",
    "\n",
    "# Display the styled table (only works in Jupyter or a notebook interface)\n",
    "metrics_df.set_caption(\"Regression Models Performance Comparison\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
