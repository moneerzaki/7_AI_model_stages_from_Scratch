{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# house prices dataset \n",
    "# https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./house_prices_ready_analysis_stage23.csv\")\n",
    "df = df.iloc[:, 1: ]\n",
    "# df = pd.read_csv(\"./house-prices-advanced-regression-techniques/train.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1080 samples\n",
      "Validation set: 135 samples\n",
      "Testing set: 136 samples\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('SalePrice', axis=1)  # Replace 'SalePrice' with your target variable\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Step 1: Split the data into trainin\n",
    "# g (90%) and temporary set (10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the temporary set into validation (50% of temp) and testing (50% of temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes to ensure correct splitting\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression model: \n",
      "--------------------------- \n",
      "Root Mean Squared Error (RMSE): 26334.66931903928\n",
      "Mean Absolute Error (MAE): 19158.459423223543\n",
      "R² Score: 0.8302912687775024\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 2: Train the model using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae_linear = mean_absolute_error(y_test, y_pred)\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"linear regression model: \")\n",
    "print(\"--------------------------- \")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_linear}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_linear}\")\n",
    "print(f\"R² Score: {r2_linear}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we took the average of RMSE and MAE we can say that on a prediction scale there is around 22,000 dollars variance or error range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trial to see if there is overfitting in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression model: \n",
      "--------------------------- \n",
      "Root Mean Squared Error (RMSE): 23992.202848990168\n",
      "Mean Absolute Error (MAE): 17150.462404685582\n",
      "R² Score: 0.8604958212172376\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 2: Train the model using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# Step 4: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "mae_linear = mean_absolute_error(y_train, y_pred)\n",
    "r2_linear = r2_score(y_train, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"linear regression model: \")\n",
    "print(\"--------------------------- \")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_linear}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_linear}\")\n",
    "print(f\"R² Score: {r2_linear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best hyperparameters: {'alpha': 10}\n",
      "Ridge Regression model with tuned hyperparameters: \n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 26323.04930744234\n",
      "Mean Absolute Error (MAE): 19130.826094331842\n",
      "R² Score: 0.8304410016130064\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    # 'fit_intercept': [True, False],\n",
    "    # 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "    # 'max_iter': [1000, 5000]\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Ridge model with the best hyperparameters\n",
    "best_ridge = Ridge(**best_params)\n",
    "best_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the testing set\n",
    "y_pred_ridge = best_ridge.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Ridge Regression model with tuned hyperparameters: \")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge}\")\n",
    "print(f\"R² Score: {r2_ridge}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lasso regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.500e+11, tolerance: 3.602e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.402e+11, tolerance: 3.551e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.596e+11, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.491e+11, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.403e+11, tolerance: 3.445e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+11, tolerance: 3.602e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.400e+11, tolerance: 3.551e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.595e+11, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+11, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.401e+11, tolerance: 3.445e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+11, tolerance: 3.602e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.373e+11, tolerance: 3.551e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.587e+11, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+11, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.386e+11, tolerance: 3.445e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+11, tolerance: 3.602e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.095e+11, tolerance: 3.551e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.502e+11, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e+11, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.237e+11, tolerance: 3.445e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.738e+09, tolerance: 3.602e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+09, tolerance: 3.551e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.584e+11, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.861e+11, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.575e+10, tolerance: 3.445e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 100}\n",
      "Lasso Regression model with tuned hyperparameters: \n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 26336.776486077884\n",
      "Mean Absolute Error (MAE): 19119.5048081803\n",
      "R² Score: 0.8302641092223836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.929e+08, tolerance: 3.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+08, tolerance: 3.538e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Lasso regression\n",
    "lasso = Lasso()\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],  # Alpha values for L1 regularization\n",
    "    # 'fit_intercept': [True, False],            # Whether to fit an intercept\n",
    "    # 'max_iter': [1000, 5000],                  # Number of iterations for convergence\n",
    "    # 'tol': [1e-4, 1e-3, 1e-2]                 # Tolerance for stopping criteria\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Lasso model with the best hyperparameters\n",
    "best_lasso = Lasso(**best_params)\n",
    "best_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the validation set\n",
    "y_pred_lasso = best_lasso.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Lasso Regression model with tuned hyperparameters: \")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso}\")\n",
    "print(f\"R² Score: {r2_lasso}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best hyperparameters: {'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "Decision Tree Regression model with tuned hyperparameters: \n",
      "---------------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 28225.508669425286\n",
      "Mean Absolute Error (MAE): 16898.914698222496\n",
      "R² Score: 0.875628380359854\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Decision Tree Regression and hyperparameter tuning\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize Decision Tree Regressor\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, None],                # Depth of the tree\n",
    "    'min_samples_split': [2, 10, 20],                  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 5, 10],                    # Minimum number of samples required to be at a leaf node\n",
    "    # 'max_features': [None, 'auto', 'sqrt', 'log2'],     # Number of features to consider at each split\n",
    "    # 'criterion': ['mse', 'friedman_mse']                # Quality measure of the split\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Decision Tree model with the best hyperparameters\n",
    "best_tree = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the validation set\n",
    "y_pred_tree = best_tree.predict(X_train)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_train, y_pred_tree))\n",
    "mae_tree = mean_absolute_error(y_train, y_pred_tree)\n",
    "r2_tree = r2_score(y_train, y_pred_tree)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Decision Tree Regression model with tuned hyperparameters: \")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_tree}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_tree}\")\n",
    "print(f\"R² Score: {r2_tree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Method          RMSE           MAE  R² Score\n",
      "0  Linear Regression  28410.892119  18544.830309  0.880137\n",
      "1   Ridge Regression  31340.737451  18934.365768  0.854141\n",
      "2   Lasso Regression  25052.598313  15519.910027  0.906799\n",
      "3      Decision Tree  28225.508669  16898.914698  0.875628\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1f823\">\n",
       "  <caption>Regression Models Performance Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1f823_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
       "      <th id=\"T_1f823_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "      <th id=\"T_1f823_level0_col2\" class=\"col_heading level0 col2\" >MAE</th>\n",
       "      <th id=\"T_1f823_level0_col3\" class=\"col_heading level0 col3\" >R² Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1f823_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1f823_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_1f823_row0_col1\" class=\"data row0 col1\" >28410.89</td>\n",
       "      <td id=\"T_1f823_row0_col2\" class=\"data row0 col2\" >18544.83</td>\n",
       "      <td id=\"T_1f823_row0_col3\" class=\"data row0 col3\" >0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f823_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1f823_row1_col0\" class=\"data row1 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_1f823_row1_col1\" class=\"data row1 col1\" >31340.74</td>\n",
       "      <td id=\"T_1f823_row1_col2\" class=\"data row1 col2\" >18934.37</td>\n",
       "      <td id=\"T_1f823_row1_col3\" class=\"data row1 col3\" >0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f823_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1f823_row2_col0\" class=\"data row2 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_1f823_row2_col1\" class=\"data row2 col1\" >25052.60</td>\n",
       "      <td id=\"T_1f823_row2_col2\" class=\"data row2 col2\" >15519.91</td>\n",
       "      <td id=\"T_1f823_row2_col3\" class=\"data row2 col3\" >0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f823_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1f823_row3_col0\" class=\"data row3 col0\" >Decision Tree</td>\n",
       "      <td id=\"T_1f823_row3_col1\" class=\"data row3 col1\" >28225.51</td>\n",
       "      <td id=\"T_1f823_row3_col2\" class=\"data row3 col2\" >16898.91</td>\n",
       "      <td id=\"T_1f823_row3_col3\" class=\"data row3 col3\" >0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1886b674a70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace these with the actual numeric (float) values from your experiments\n",
    "metrics_data = {\n",
    "    'Method': ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Decision Tree'],\n",
    "    'RMSE': [rmse_linear, rmse_ridge, rmse_lasso, rmse_tree],  # Ensure these are float values\n",
    "    'MAE': [mae_linear, mae_ridge, mae_lasso, mae_tree],        # Ensure these are float values\n",
    "    'R² Score': [r2_linear, r2_ridge, r2_lasso, r2_tree]        # Ensure these are float values\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Print the table\n",
    "print(metrics_df)\n",
    "\n",
    "# If you want to format the table with two decimal places, use this:\n",
    "metrics_df = metrics_df.style.format({\"RMSE\": \"{:.2f}\", \"MAE\": \"{:.2f}\", \"R² Score\": \"{:.2f}\"})\n",
    "\n",
    "# Display the styled table (only works in Jupyter or a notebook interface)\n",
    "metrics_df.set_caption(\"Regression Models Performance Comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice linear regression, ridge, and lasso regressions are almost the same results. The ridge is much closer to the linear regression which makes sense because the alpha value, the hyperparameter, for ridge is only 10 which means there is not a big bias to the slope change than the linear regression. While the alpha value for the lasso regression is much bigger, value of 100, which means more slope bias than the linear regression and more difference in the prediction. \n",
    "\n",
    "Linear regression is the best that means the training and testing datasets are extremely close to each other, almost the same. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about training on the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv(\"./house_prices_ready_analysis_stage23.csv\")\n",
    "data = pd.read_csv(\"./house-prices-advanced-regression-techniques/train.csv\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass         0\n",
      "MSZoning           0\n",
      "LotFrontage      259\n",
      "LotArea            0\n",
      "Street             0\n",
      "                ... \n",
      "MoSold             0\n",
      "YrSold             0\n",
      "SaleType           0\n",
      "SaleCondition      0\n",
      "SalePrice          0\n",
      "Length: 80, dtype: int64\n",
      "(1460, 80) \n",
      "\n",
      " **************** \n",
      "Those are the columns with null values almost greater than 50% to drop the whole feature column from the entire dataset: \n",
      " Alley          93.767123\n",
      "MasVnrType     59.726027\n",
      "FireplaceQu    47.260274\n",
      "PoolQC         99.520548\n",
      "Fence          80.753425\n",
      "MiscFeature    96.301370\n",
      "dtype: float64\n",
      "(1460, 74) \n",
      "\n",
      " **************** \n",
      "Those are the columns with null values almost less than 5% \n",
      "I replaced with the mode imputation (most repetitive value in the column): \n",
      "\n",
      " Index(['MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
      "       'BsmtFinType2', 'Electrical', 'GarageType', 'GarageYrBlt',\n",
      "       'GarageFinish', 'GarageQual', 'GarageCond'],\n",
      "      dtype='object')\n",
      "(1460, 74)  \n",
      "\n",
      " ************ \n",
      "(1460, 74) LotFrontage    259\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monee\\AppData\\Local\\Temp\\ipykernel_21344\\939326224.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  pre[column].fillna(pre[column].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# dropping IID columns: \n",
    "pre = data.copy()\n",
    "pre = pre.drop(['Id'], axis=1)\n",
    "\n",
    "# dropping null values: \n",
    "print(pre.isnull().sum())\n",
    "\n",
    "# test = prepro_data.copy()\n",
    "null_counts = pre.isnull().mean()*100\n",
    "print(pre.shape, \"\\n\\n **************** \")\n",
    "\n",
    "# drop those features having null values more than 500 \n",
    "columns_with_50nulls = null_counts[null_counts >= 45]\n",
    "pre = pre.drop(columns = columns_with_50nulls.index)\n",
    "print(\"Those are the columns with null values almost greater than 50% to drop the whole feature column from the entire dataset: \\n\", columns_with_50nulls)\n",
    "print(pre.shape , \"\\n\\n **************** \")\n",
    "\n",
    "\n",
    "# Identify columns with fewer than 5 but more than 0 null values\n",
    "columns_with_nulls = null_counts[(null_counts < 6) & (null_counts > 0)].index\n",
    "# Either drop the rows \n",
    "# # test = test.dropna(subset = columns_with_nulls)\n",
    "# Or fill them with the most repetitive value in the column which is more preferable to not decrease the dataset. \n",
    "for column in columns_with_nulls:\n",
    "    pre[column].fillna(pre[column].mode()[0], inplace=True)\n",
    "# columns_with_nulls = columns_with_nulls[columns_with_nulls > 0]\n",
    "print(\"Those are the columns with null values almost less than 5% \\nI replaced with the mode imputation (most repetitive value in the column): \\n\\n\", columns_with_nulls)\n",
    "print(pre.shape, \" \\n\\n ************ \")\n",
    "\n",
    "# \n",
    "null_counts = pre.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "print(pre.shape, null_counts)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pre.isnull().sum().sum())\n",
    "pre = pre.dropna()\n",
    "print(pre.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encdoing categorical features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1201, 260)\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(pre)\n",
    "\n",
    "# Convert all boolean values (True/False) to integers (1/0)\n",
    "df = df.astype({col: 'int' for col in df.select_dtypes(include=['bool']).columns})\n",
    "\n",
    "# dropping non-numerical features\n",
    "# df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all non-numerical features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 960 samples\n",
      "Validation set: 120 samples\n",
      "Testing set: 121 samples\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('SalePrice', axis=1)  # Replace 'SalePrice' with your target variable\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Step 1: Split the data into trainin\n",
    "# g (90%) and temporary set (10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the temporary set into validation (50% of temp) and testing (50% of temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes to ensure correct splitting\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression model: \n",
      "--------------------------- \n",
      "Root Mean Squared Error (RMSE): 28410.892118550353\n",
      "Mean Absolute Error (MAE): 18544.8303086884\n",
      "R² Score: 0.8801372479249101\n"
     ]
    }
   ],
   "source": [
    "# linear regression on hte whole dataset columns\n",
    "\n",
    "# Step 1: Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 2: Train the model using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the validation set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae_linear = mean_absolute_error(y_test, y_pred)\n",
    "r2_linear = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"linear regression model: \")\n",
    "print(\"--------------------------- \")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_linear}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_linear}\")\n",
    "print(f\"R² Score: {r2_linear}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best hyperparameters: {'alpha': 10}\n",
      "Ridge Regression model with tuned hyperparameters: \n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 31340.737451250054\n",
      "Mean Absolute Error (MAE): 18934.365767706655\n",
      "R² Score: 0.8541411028841343\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "    # 'fit_intercept': [True, False],\n",
    "    # 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "    # 'max_iter': [1000, 5000]\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Ridge model with the best hyperparameters\n",
    "best_ridge = Ridge(**best_params)\n",
    "best_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the testing set\n",
    "y_pred_ridge = best_ridge.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Ridge Regression model with tuned hyperparameters: \")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge}\")\n",
    "print(f\"R² Score: {r2_ridge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.748e+10, tolerance: 4.182e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.359e+10, tolerance: 5.202e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.216e+11, tolerance: 5.013e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.338e+10, tolerance: 5.017e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.302e+11, tolerance: 5.171e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.239e+11, tolerance: 4.182e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.598e+10, tolerance: 5.202e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.060e+10, tolerance: 5.013e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.468e+10, tolerance: 5.017e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.093e+11, tolerance: 5.171e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.310e+11, tolerance: 4.182e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.571e+11, tolerance: 5.202e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+10, tolerance: 5.013e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e+11, tolerance: 5.017e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.334e+10, tolerance: 5.171e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.317e+11, tolerance: 4.182e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.531e+11, tolerance: 5.202e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+11, tolerance: 5.013e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.634e+10, tolerance: 5.017e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.080e+11, tolerance: 5.171e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+09, tolerance: 4.182e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\monee\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.590e+08, tolerance: 5.171e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 100}\n",
      "Lasso Regression model with tuned hyperparameters: \n",
      "--------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 25052.598313042316\n",
      "Mean Absolute Error (MAE): 15519.910027131218\n",
      "R² Score: 0.9067991113910054\n"
     ]
    }
   ],
   "source": [
    "# lasso regression \n",
    "\n",
    "# Step 1: Initialize Lasso regression\n",
    "lasso = Lasso()\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],  # Alpha values for L1 regularization\n",
    "    # 'fit_intercept': [True, False],            # Whether to fit an intercept\n",
    "    # 'max_iter': [1000, 5000],                  # Number of iterations for convergence\n",
    "    # 'tol': [1e-4, 1e-3, 1e-2]                 # Tolerance for stopping criteria\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Lasso model with the best hyperparameters\n",
    "best_lasso = Lasso(**best_params)\n",
    "best_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the validation set\n",
    "y_pred_lasso = best_lasso.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Lasso Regression model with tuned hyperparameters: \")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lasso}\")\n",
    "print(f\"R² Score: {r2_lasso}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best hyperparameters: {'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "Decision Tree Regression model with tuned hyperparameters: \n",
      "---------------------------------------------------------\n",
      "Root Mean Squared Error (RMSE): 42332.54562087181\n",
      "Mean Absolute Error (MAE): 27722.23726465462\n",
      "R² Score: 0.7338887266689347\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Decision Tree Regression and hyperparameter tuning\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize Decision Tree Regressor\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, None],                # Depth of the tree\n",
    "    'min_samples_split': [2, 10, 20],                  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 5, 10],                    # Minimum number of samples required to be at a leaf node\n",
    "    # 'max_features': [None, 'auto', 'sqrt', 'log2'],     # Number of features to consider at each split\n",
    "    # 'criterion': ['mse', 'friedman_mse']                # Quality measure of the split\n",
    "}\n",
    "\n",
    "# Step 3: Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Step 4: Train the model using the training set and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 6: Train the Decision Tree model with the best hyperparameters\n",
    "best_tree = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the validation set\n",
    "y_pred_tree = best_tree.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model using RMSE, MAE, and R²\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Decision Tree Regression model with tuned hyperparameters: \")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_tree}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_tree}\")\n",
    "print(f\"R² Score: {r2_tree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Method          RMSE           MAE  R² Score\n",
      "0  Linear Regression  28410.892119  18544.830309  0.880137\n",
      "1   Ridge Regression  31340.737451  18934.365768  0.854141\n",
      "2   Lasso Regression  25052.598313  15519.910027  0.906799\n",
      "3      Decision Tree  42332.545621  27722.237265  0.733889\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cee7e\">\n",
       "  <caption>Regression Models Performance Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cee7e_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
       "      <th id=\"T_cee7e_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "      <th id=\"T_cee7e_level0_col2\" class=\"col_heading level0 col2\" >MAE</th>\n",
       "      <th id=\"T_cee7e_level0_col3\" class=\"col_heading level0 col3\" >R² Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cee7e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cee7e_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_cee7e_row0_col1\" class=\"data row0 col1\" >28410.89</td>\n",
       "      <td id=\"T_cee7e_row0_col2\" class=\"data row0 col2\" >18544.83</td>\n",
       "      <td id=\"T_cee7e_row0_col3\" class=\"data row0 col3\" >0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cee7e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cee7e_row1_col0\" class=\"data row1 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_cee7e_row1_col1\" class=\"data row1 col1\" >31340.74</td>\n",
       "      <td id=\"T_cee7e_row1_col2\" class=\"data row1 col2\" >18934.37</td>\n",
       "      <td id=\"T_cee7e_row1_col3\" class=\"data row1 col3\" >0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cee7e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cee7e_row2_col0\" class=\"data row2 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_cee7e_row2_col1\" class=\"data row2 col1\" >25052.60</td>\n",
       "      <td id=\"T_cee7e_row2_col2\" class=\"data row2 col2\" >15519.91</td>\n",
       "      <td id=\"T_cee7e_row2_col3\" class=\"data row2 col3\" >0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cee7e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cee7e_row3_col0\" class=\"data row3 col0\" >Decision Tree</td>\n",
       "      <td id=\"T_cee7e_row3_col1\" class=\"data row3 col1\" >42332.55</td>\n",
       "      <td id=\"T_cee7e_row3_col2\" class=\"data row3 col2\" >27722.24</td>\n",
       "      <td id=\"T_cee7e_row3_col3\" class=\"data row3 col3\" >0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x188685ca210>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace these with the actual numeric (float) values from your experiments\n",
    "metrics_data = {\n",
    "    'Method': ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Decision Tree'],\n",
    "    'RMSE': [rmse_linear, rmse_ridge, rmse_lasso, rmse_tree],  # Ensure these are float values\n",
    "    'MAE': [mae_linear, mae_ridge, mae_lasso, mae_tree],        # Ensure these are float values\n",
    "    'R² Score': [r2_linear, r2_ridge, r2_lasso, r2_tree]        # Ensure these are float values\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Print the table\n",
    "print(metrics_df)\n",
    "\n",
    "# If you want to format the table with two decimal places, use this:\n",
    "metrics_df = metrics_df.style.format({\"RMSE\": \"{:.2f}\", \"MAE\": \"{:.2f}\", \"R² Score\": \"{:.2f}\"})\n",
    "\n",
    "# Display the styled table (only works in Jupyter or a notebook interface)\n",
    "metrics_df.set_caption(\"Regression Models Performance Comparison\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
